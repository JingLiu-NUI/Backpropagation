{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_addmorehiddenlayer.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "FGIAdawLaYy3",
        "colab_type": "code",
        "outputId": "8ae06f17-003c-4d43-8811-27bc39653274",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!Is \"/content/drive/My Drive/Colab Notebooks\"\n",
        "inputfile = '/content/drive/My Drive/Colab Notebooks/test_batch'\n",
        "dataset = pd.read_csv('/content/drive/My Drive/Colab Notebooks/circles500.csv')\n",
        "buf=open(inputfile, \"rb\")\n",
        "from scipy.misc import imsave\n",
        "import numpy as np\n",
        "import pickle as p\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as plimg\n",
        "from PIL import Image\n",
        "import pickle\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/bin/bash: Is: command not found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MlvZX3kDM9sr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "random.seed(0)\n",
        "import csv          \n",
        "#reference http://www.youtube.com/watch?v=aVId8KMsdUU&feature=BFa&list=LLldMCkmXl4j9_v0HeKdNcRA\n",
        "# https://blog.csdn.net/u012722531/article/details/80026566 \n",
        "#this is the neural network calss I defined for fitting the image data to classify frogs and dogs\n",
        "class neural_network:\n",
        "    result1=[]\n",
        "    #initialize all parameters that in should be set on the nureal network \n",
        "    def __init__(self,input_vector,input_size,hidden_nodes,learning_rate,label):\n",
        "        weight=[]\n",
        "        bias=[]\n",
        "        #input data\n",
        "        self.input_vector=input_vector\n",
        "        #input size\n",
        "        self.input_size=input_size\n",
        "        #nodes on hidden layer\n",
        "        self.hidden_nodes=hidden_nodes\n",
        "        #learning rate\n",
        "        self.learning_rate=learning_rate\n",
        "        #label on the training data\n",
        "        self.label=label\n",
        "        size=([input_size,hidden_nodes,hidden_nodes,1])\n",
        "        #https://blog.csdn.net/leo_sheng/article/details/80741789  how to use random.uniform( ) \n",
        "        #this is initialization for weights and bias, using random.uniform to get the inital weigts and bias \n",
        "        #for all layers from range -1 to 1.\n",
        "        for l1,l2 in zip(size[:-1],size[1:]):\n",
        "            w=np.zeros((l1,l2), dtype=float, order='C')\n",
        "            for i in range(l1):\n",
        "                for j in range(l2):\n",
        "                    w[i][j]=random.uniform(-1.0, 1.0)\n",
        "            weight.append(w)\n",
        "        for y in size[1:]:\n",
        "            b=np.zeros((y,1), dtype=float, order='C')\n",
        "            for i in range(b.shape[0]): \n",
        "                b[i]=random.uniform(-1.0, 1.0) \n",
        "            bias.append(b)\n",
        "        #weights of all layers\n",
        "        self.weight=weight\n",
        "        #bia of all layers\n",
        "        self.bias=bias \n",
        "    #sigmoid function through using the formual of sigmoid\n",
        "    #reference:https://blog.csdn.net/tyhj_sf/article/details/79932893\n",
        "    def sigmoid(self,z):\n",
        "        return 1.0/(1.0+np.exp(-z))\n",
        "    # derivate of sigmoid function: z represent sigmoid(x), after execute the \n",
        "    # feed forward function , the x vlaue already become sigmoid(x), so here using\n",
        "    # z*z(1-z)only, which means sigmoid(x)*(1-sigmoid(x))\n",
        "    def derivative_sigmoid(self, z):\n",
        "        return z*(1-z)\n",
        "\n",
        "    # the feed forward is caculate the initial output using the function of h=x*w+b\n",
        "    # a=sigmoid(h), using result list add all output of each layer in case of next step to \n",
        "    # update the weights.\n",
        "    def feed_forward(self,input_vector):\n",
        "        results=[]\n",
        "        results.append(input_vector)\n",
        "        for w,b in zip(self.weight,self.bias):\n",
        "            input_vector=self.sigmoid(np.dot(input_vector,w)+b.T)\n",
        "            results.append(input_vector)\n",
        "        return results\n",
        "   #back_propagattion used for updating all layer's weights, here using the formula on the class\n",
        "    def back_prop(self,input_vector,y):\n",
        "        for iteration in range(1):\n",
        "          #firstly get the output from the network used for update the previous layer's weight\n",
        "            output_result=self.feed_forward(input_vector)\n",
        "            #the error only used real value-predicted value\n",
        "            error=y-output_result[-1]\n",
        "            ##output layer\n",
        "            #using the formula form lecture, derivation_sigmoid(ao)*error\n",
        "            delta=self.derivative_sigmoid(output_result[-1])*error \n",
        "            #using output_result * delta\n",
        "            adjustment = np.dot(output_result[-1-1].T,delta)\n",
        "            #using weight+=adjustment*lr\n",
        "            self.weight[-1]=self.weight[-1]+adjustment*self.learning_rate\n",
        "            #updata weights from hidden layer using the last updated weights,\n",
        "            #here for loop depends on how many hidden layers you have, always \n",
        "            #update the weights and bias from by behind one layers weight\n",
        "            for l in range(2,4):\n",
        "                z = output_result[-l]\n",
        "                # here get derivative of previous layers output value f'(zj)\n",
        "                sp = self.derivative_sigmoid(z)\n",
        "                # using function  f'(zj) · Σk Wj,k \n",
        "                delta = np.dot(delta,self.weight[-l+1].T) * sp\n",
        "                #using function ∆j = f'(zj) · Σk Wj,k ∆k \n",
        "                adjustment = np.dot(output_result[-l-1].T,delta)\n",
        "                #using the function Wi.j ← Wi,j + ai ∆j\n",
        "                self.weight[-l]=self.weight[-l]+adjustment*self.learning_rate\n",
        "        return error\n",
        "    # the fit function is used for pass the parameter to back-pro and print the loss value\n",
        "    def fit(self):\n",
        "        for k in range(1000):\n",
        "            for i in range(len(self.input_vector)):\n",
        "                feature_vector=np.array([self.input_vector[i]])\n",
        "                label_vector=np.array([self.label[i]])\n",
        "                error=self.back_prop(feature_vector,label_vector)\n",
        "            if k % 50==0:\n",
        "                print('loss:',error)\n",
        "\n",
        "       "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k_oBMUS1p53A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 单层\n",
        "from sklearn.model_selection import train_test_split\n",
        "def classification_prediction(x):\n",
        "    predict_label=[]\n",
        "    for i in x:\n",
        "        #if value is bigger then 0.5 then is dog. if it is 6 then is frog\n",
        "        if i[0] >0.5:\n",
        "           predict_label.append(1)\n",
        "        elif i[0]<0.5:\n",
        "             predict_label.append(0)\n",
        "          \n",
        "    return predict_label  \n",
        "def accuracy_test(a,b):\n",
        "    right = []\n",
        "    for i in range(len(a)):\n",
        "        if a[i]==b[i]:\n",
        "           right.append(1)\n",
        "    accuracy = len(right)/len(a)\n",
        "    return accuracy  \n",
        "  # onvert the array to a float vector of 1024\n",
        "def convert_to_1024_vector(a):\n",
        "    list_1024 = []\n",
        "    #get the first color \n",
        "    for j in range(len(a[1])):\n",
        "        for i in range(len(a[1][j])):\n",
        "          #convert it into a float vector of 1024\n",
        "            list_1024.append(a[1][j][i]/255.0)\n",
        "            \n",
        "    return list_1024\n",
        "#use unpickle to get the dataset of image\n",
        "# referenfe: https://www.cnblogs.com/hans209/p/6919851.html\n",
        "def unpickle(file):\n",
        "    train_x=[]\n",
        "    train_y=[]\n",
        "    with open(file, 'rb') as fo:\n",
        "        datadict = pickle.load(fo, encoding='latin1')\n",
        "        X = datadict['data']\n",
        "        Y = datadict['labels']\n",
        "        X = X.reshape(10000, 3, 32, 32)\n",
        "        #get only one color     \n",
        "        for i in range(len(Y)):\n",
        "          # get image which label is 5 06 6(dog and frog)\n",
        "            if  np.any(Y[i] ==5) or np.any(Y[i]==6):\n",
        "                train_set_x_1024 =convert_to_1024_vector(X[i])\n",
        "                if Y[i] == 5:\n",
        "                    train_y.append(1)\n",
        "                elif  Y[i] == 6:\n",
        "                    train_y.append(0)\n",
        "                train_x.append(train_set_x_1024)\n",
        "    data_x = np.array(train_x)\n",
        "    data_y = np.array(train_y)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(data_x[1:5], data_y[1:5], test_size=0.4,random_state=0)\n",
        "    print(len(X_train))\n",
        "    ne=neural_network(X_train,1024,6,0.1,y_train)\n",
        "    ne.fit()\n",
        "    pe=[]\n",
        "    correct=0\n",
        "    for i in range(len(X_test)):\n",
        "        pre=ne.feed_forward(X_test[i])\n",
        "        print(\"21111111111\",pre[-1])\n",
        "        if pre[-1]>0.5:\n",
        "            pe.append(1)\n",
        "        else:\n",
        "            pe.append(0)\n",
        "    for i in range(len(pe)):\n",
        "        print(y_test[i])\n",
        "        if float(pe[i])==y_test[i]:\n",
        "            correct=correct+1\n",
        "    accuracy=correct/float(len(pe))\n",
        "    \n",
        "    predict_simple_label = classification_prediction(prediction_simple)\n",
        "    cc = accuracy_test(predict_simple_label,y_train)\n",
        "    print(\"##############\",cc)\n",
        "    # referenfe: https://www.cnblogs.com/hans209/p/6919851.html\n",
        "test_data = unpickle(\"/content/drive/My Drive/Colab Notebooks/test_batch\")\n",
        "load_CIFAR_Labels=unpickle(\"/content/drive/My Drive/Colab Notebooks/data_batch_1\")\n",
        "def load_CIFAR_Labels(filename):\n",
        "    with open(filename, 'rb') as f:\n",
        "        lines = [x for x in f.readlines()]\n",
        "if __name__ == \"__main__\":\n",
        "    load_CIFAR_Labels(\"/content/drive/My Drive/Colab Notebooks/batches.meta\")\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_QrSrg7WkVAE",
        "colab_type": "code",
        "outputId": "257bff5f-d278-476b-95af-285cf984eac4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "cell_type": "code",
      "source": [
        "#get dataset \n",
        "# referenfe: https://www.cnblogs.com/hans209/p/6919851.html\n",
        "def unpickle(file):\n",
        "    with open(file, 'rb')as f:\n",
        "#        datadict = p.load(f)\n",
        "        datadict = p.load(f,encoding='latin1')\n",
        "        X = datadict['data']\n",
        "        Y = datadict['labels']\n",
        "        X = X.reshape(10000, 3, 32, 32)\n",
        "        Y = np.array(Y)\n",
        "        return  X, Y\n",
        "# onvert the array to a float vector of 1024\n",
        "def convert_to_1024_vector(a):\n",
        "    \n",
        "    list_1024 = []\n",
        "    for j in range(len(a[0])):\n",
        "        for i in range(len(a[j])):\n",
        "            list_1024.append(a[j][i]/255.0)\n",
        "    return list_1024\n",
        "def load_CIFAR_Labels(filename):\n",
        "    with open(filename, 'rb') as f:\n",
        "        lines = [x for x in f.readlines()]\n",
        "def image_to_vector(imgX,imgY):\n",
        "    X=[]\n",
        "    Y=[]\n",
        "    for i in range(imgX.shape[0]):\n",
        "        if  np.any(imgY[i] ==5) or np.any(imgY[i]==6):\n",
        "            imgs = imgX[i]\n",
        "            label = imgY[i]\n",
        "        # get three versions from dataset                \n",
        "        # from data to image\n",
        "            color_0 = Image.fromarray(imgs[0])\n",
        "            color_1 = Image.fromarray(imgs[1])\n",
        "            color_2 = Image.fromarray(imgs[2])\n",
        "            #merge three colors togather, then use convert to make it gray\n",
        "            # fererence: https://vimsky.com/article/3729.html\n",
        "            img = Image.merge(\"RGB\", (color_0, color_1, color_2)).convert(\"L\")\n",
        "            # call \"convert_to_1024_vector\" to convert array into a 1024 float vector\n",
        "            image_vector_1024 = convert_to_1024_vector(np.array(img))\n",
        "            X.append(image_vector_1024)\n",
        "            if imgY[i]==5:\n",
        "               Y.append(1)\n",
        "            elif imgY[i]==6:\n",
        "                 Y.append(0)\n",
        "          \n",
        "    X=np.array(X)\n",
        "    Y=np.array(Y)\n",
        "    return X,Y\n",
        " # \n",
        "def classification_prediction(x):\n",
        "    predict_label=[]\n",
        "    for i in x:\n",
        "        \n",
        "        if i[0] >0.5:\n",
        "           predict_label.append(1)\n",
        "        elif i[0]<0.5:\n",
        "             predict_label.append(0)\n",
        "          \n",
        "    return predict_label\n",
        "  \n",
        "def accuracy_test(a,b):\n",
        "    right = []\n",
        "    for i in range(len(a)):\n",
        "        if a[i]==b[i]:\n",
        "           right.append(1)\n",
        "    accuracy = len(right)/len(a)\n",
        "    return accuracy\n",
        "  \n",
        "if __name__ == \"__main__\":\n",
        "    load_CIFAR_Labels(\"/content/drive/My Drive/Colab Notebooks/batches.meta\")\n",
        "    test_imgX, test_imgY = unpickle(\"/content/drive/My Drive/Colab Notebooks/test_batch\")\n",
        "    train_imgX, train_imgY= unpickle(\"/content/drive/My Drive/Colab Notebooks/data_batch_1\")\n",
        "    #here I using all data to testing for all traing data from data_batch_1, tseting data from test_batch\n",
        "    X_train=train_batch_data[0]\n",
        "    y_train=train_batch_data[1]\n",
        "    X_test=test_batch_data[0]\n",
        "    y_test=test_batch_data[1]\n",
        "#     still can using sample to random select data\n",
        "#     X_train=random.sample(list(train_batch_data[0]),1500) \n",
        "#     y_train=random.sample(list(train_batch_data[1]),1500) \n",
        "#     X_test=random.sample(list(test_batch_data[0]),400) \n",
        "#     y_test=random.sample(list(test_batch_data[1]),400)\n",
        "    #this is for data training, the input data, input size, hidden layer size and learning rate and label.\n",
        "    ne=neural_network(X_train,1024,50,0.05,y_train)\n",
        "    ne.fit()\n",
        "    pe=[]\n",
        "    correct=0\n",
        "    #get the accuracy, if it's correct, then total correct number +1,using \n",
        "    #correct number/total\n",
        "    for i in range(len(X_test)):\n",
        "        pre=ne.feed_forward(X_test[i])\n",
        "        if pre[-1]>0.5:\n",
        "            pe.append(1)\n",
        "        else:\n",
        "            pe.append(0)\n",
        "    for i in range(len(pe)):\n",
        "        if float(pe[i])==y_test[i]:\n",
        "            correct=correct+1\n",
        "    accuracy=correct/float(len(pe))\n",
        "    print(\"the accuracy of two hidden layers\",accuracy)\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss: [[0.76117248]]\n",
            "loss: [[0.76737829]]\n",
            "loss: [[0.1120672]]\n",
            "loss: [[0.03559981]]\n",
            "loss: [[0.00200097]]\n",
            "loss: [[7.45596717e-05]]\n",
            "loss: [[0.00011762]]\n",
            "loss: [[5.2593559e-05]]\n",
            "loss: [[3.91342419e-05]]\n",
            "loss: [[3.09615144e-05]]\n",
            "loss: [[2.36710515e-05]]\n",
            "loss: [[1.06073747e-05]]\n",
            "loss: [[6.56022268e-06]]\n",
            "loss: [[4.10796285e-06]]\n",
            "loss: [[3.10564055e-06]]\n",
            "loss: [[2.34511264e-06]]\n",
            "loss: [[4.5870369e-06]]\n",
            "loss: [[2.41049756e-06]]\n",
            "loss: [[1.84268828e-06]]\n",
            "loss: [[1.55828602e-06]]\n",
            "the accuracy of two hidden layers 0.7015\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HfT1jrBBrhxv",
        "colab_type": "code",
        "outputId": "8a98c433-fc2d-4d14-d191-0e8e161872ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#from the confusionmatricx we can see how many should be positive and we predict to positive and negitive\n",
        "#how many should be negitive and we predict it to negative\n",
        "def  confusion_matricx(y_true, y_pred):\n",
        "    TP = 0\n",
        "    TN = 0\n",
        "    FP = 0\n",
        "    FN = 0\n",
        "    for x in range(len(y_true)):\n",
        "        if y_true[x] == 0 and y_pred[x] == 0:\n",
        "            TN += 1\n",
        "        if y_true[x] == 0 and y_pred[x] == 1:\n",
        "            FP += 1\n",
        "        if y_true[x] == 1 and y_pred[x] == 1:\n",
        "            TP += 1\n",
        "        if y_true[x] == 1 and y_pred[x] == 0:\n",
        "            FN += 1\n",
        "    cm = [[TN, FP],[FN, TP]]\n",
        "    cm=np.array(cm)\n",
        "    return cm\n",
        "comfuse = confusion_matricx(y_test,pe)\n",
        "fig, ax = plt.subplots(figsize=(2.5, 2.5))\n",
        "ax.matshow(comfuse, cmap=plt.cm.Blues, alpha=0.3)\n",
        "for i in range(comfuse.shape[0]):\n",
        "    for j in range(comfuse.shape[1]):\n",
        "        ax.text(x=j, y=i, s=comfuse[i, j], va='center', ha='center')\n",
        "plt.xlabel('predicted label')\n",
        "plt.ylabel('true label')\n",
        "plt.show()\n",
        "\n",
        "def precision_score1(y_true, y_pred):\n",
        "    TP = 0\n",
        "    TN = 0\n",
        "    FP = 0\n",
        "    FN = 0\n",
        "    for x in range(len(y_true)):\n",
        "        if y_true[x] == 0 and y_pred[x] == 0:\n",
        "            TN += 1\n",
        "        if y_true[x] == 0 and y_pred[x] == 1:\n",
        "            FP += 1\n",
        "        if y_true[x] == 1 and y_pred[x] == 1:\n",
        "            TP += 1\n",
        "        if y_true[x] == 1 and y_pred[x] == 0:\n",
        "            FN += 1\n",
        "        x += 1\n",
        "    p=TP/float((TP+FP))\n",
        "    return p\n",
        "  \n",
        "print('Precision: %.3f' % precision_score1(y_true=y_test, y_pred=pe))\n",
        "\n",
        "def recall_score(y_true, y_pred):\n",
        "    TP = 0\n",
        "    TN = 0\n",
        "    FP = 0\n",
        "    FN = 0\n",
        "    for x in range(len(y_true)):\n",
        "        if y_true[x] == 0 and y_pred[x] == 0:\n",
        "            TN += 1\n",
        "        if y_true[x] == 0 and y_pred[x] == 1:\n",
        "            FP += 1\n",
        "        if y_true[x] == 1 and y_pred[x] == 1:\n",
        "            TP += 1\n",
        "        if y_true[x] == 1 and y_pred[x] == 0:\n",
        "            FN += 1\n",
        "        x += 1\n",
        "    p=TP/float((TP+FN))\n",
        "    return p \n",
        "print('Recall: %.3f' % recall_score(y_true=y_test, y_pred=pe))\n",
        "\n",
        "#the F1 value is the harmonic mean of the precision rate and the recall rate\n",
        "def F1_score(y_true, y_pred):\n",
        "    P=precision_score1(y_true, y_pred)\n",
        "    R=recall_score(y_true, y_pred)\n",
        "    F=2*P*R/float(P+R)\n",
        "    return F\n",
        "print('F1: %.3f' % F1_score(y_true=y_test, y_pred=pe))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAALAAAAC1CAYAAAD/enXtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nalJgoWr/D2+ZxN0I+1I9AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 180x180 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Precision: 0.709\n",
            "Recall: 0.684\n",
            "F1: 0.696\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}